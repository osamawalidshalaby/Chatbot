from flask import Flask, request, jsonify, send_file
from flask_cors import CORS
import pandas as pd
import numpy as np
import os
import datetime
import re
import tempfile
import warnings
warnings.filterwarnings("ignore")

# Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ø«Ù‚ÙŠÙ„Ø© Ø¨Ù…ÙƒØªØ¨Ø§Øª Ø£Ø®Ù
try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
    HAS_SKLEARN = True
except ImportError:
    HAS_SKLEARN = False

try:
    from gtts import gTTS
    HAS_GTTS = True
except ImportError:
    HAS_GTTS = False

try:
    from googletrans import Translator
    HAS_TRANSLATOR = True
except ImportError:
    HAS_TRANSLATOR = False

# Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ø«Ù‚ÙŠÙ„Ø©
HAS_SENTENCE_TRANSFORMERS = False
HAS_PYGAME = False
HAS_SPEECH_RECOGNITION = False

app = Flask(__name__)
CORS(app)


class ChatbotAPI:
    """Chatbot API Ù…Ø®ÙÙ Ù…Ø¹ ÙƒÙ„ Ø§Ù„Ù…Ø²Ø§ÙŠØ§."""

    def __init__(self):
        print("ðŸ¤– Initializing Lightweight Chatbot API...")

        # File paths
        self.dataset_file = 'dataset.csv'

        # Settings
        self.confidence_threshold = 0.2  # Ø®ÙØ¶ threshold Ø¹Ù„Ø´Ø§Ù† Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
        self.auto_translate = False

        # Data storage
        self.questions = []
        self.answers = []
        self.vectorizer = None
        self.embeddings = None
        self.is_trained = False

        # Initialize components
        self.init_translator()
        self.load_data()
        self.train_model()

        print("âœ… Lightweight Chatbot API initialized successfully")

    def init_translator(self):
        """Initialize translator."""
        if HAS_TRANSLATOR:
            try:
                self.translator = Translator()
                print("ðŸŒ Translator initialized")
            except Exception as e:
                print(f"âš ï¸ Translator initialization failed: {e}")
                self.translator = None

    def detect_language(self, text):
        """Detect language using regex pattern - Ù…Ø­Ø³Ù†Ø© Ù„Ù„Ø¹Ø±Ø¨ÙŠØ©."""
        if not text or not isinstance(text, str):
            return 'en'

        # Ù†Ù…Ø· Ø¹Ø±Ø¨ÙŠ Ø´Ø§Ù…Ù„
        arabic_pattern = re.compile(
            r'[\u0600-\u06FF\u0750-\u077F\u08A0-\u08FF]+')

        # ÙƒÙ„Ù…Ø§Øª Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© Ø´Ø§Ø¦Ø¹Ø©
        english_words = ['hello', 'hi', 'how', 'what', 'when',
                         'where', 'why', 'thank', 'good', 'bye', 'yes', 'no', 'ok']
        text_lower = text.lower().strip()

        # Ø¥Ø°Ø§ Ø§Ù„Ù†Øµ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø­Ø±ÙˆÙ Ø¹Ø±Ø¨ÙŠØ©
        if arabic_pattern.search(text):
            return 'ar'
        # Ø¥Ø°Ø§ Ø§Ù„Ù†Øµ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© Ø´Ø§Ø¦Ø¹Ø©
        elif any(word in text_lower for word in english_words):
            return 'en'
        else:
            # Ø¥Ø°Ø§ Ù…ÙÙŠØ´ Ù…Ø¤Ø´Ø± ÙˆØ§Ø¶Ø­ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø¥Ø­ØµØ§Ø¡ Ø§Ù„Ø­Ø±ÙˆÙ
            arabic_chars = len(arabic_pattern.findall(text))
            total_chars = len(text)

            if arabic_chars / total_chars > 0.3:  # Ø¥Ø°Ø§ Ø£ÙƒØ«Ø± Ù…Ù† 30% Ø¹Ø±Ø¨ÙŠØ©
                return 'ar'
            else:
                return 'en'

    def load_data(self):
        """Load dataset from CSV - ÙŠØ­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø©."""
        if os.path.exists(self.dataset_file):
            try:
                # Ø­Ø§ÙˆÙ„ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù Ø¨Ø·Ø±Ù‚ Ù…Ø®ØªÙ„ÙØ©
                try:
                    df = pd.read_csv(self.dataset_file, encoding='utf-8')
                except:
                    df = pd.read_csv(self.dataset_file, encoding='utf-8', on_bad_lines='skip')
                
                # ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
                if len(df.columns) >= 2:
                    # Ø®Ø° Ø£ÙˆÙ„ Ø¹Ù…ÙˆØ¯ÙŠÙ† ÙÙ‚Ø· Ù„Ùˆ ÙÙŠ Ø£ÙƒØªØ± Ù…Ù† ÙƒØ¯Ù‡
                    if 'question' in df.columns and 'answer' in df.columns:
                        questions_col = 'question'
                        answers_col = 'answer'
                    else:
                        questions_col = df.columns[0]
                        answers_col = df.columns[1]
                        print(f"âš ï¸ Using columns: {questions_col} and {answers_col}")
                    
                    # Ù†Ø¸Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                    df_clean = df[[questions_col, answers_col]].copy()
                    df_clean = df_clean.dropna()
                    df_clean[questions_col] = df_clean[questions_col].astype(str)
                    df_clean[answers_col] = df_clean[answers_col].astype(str)
                    
                    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØµÙÙˆÙ Ø§Ù„ÙØ§Ø±ØºØ©
                    df_clean = df_clean[(df_clean[questions_col].str.strip() != '') & 
                                      (df_clean[answers_col].str.strip() != '')]
                    
                    if len(df_clean) > 0:
                        self.questions = df_clean[questions_col].tolist()
                        self.answers = df_clean[answers_col].tolist()
                        
                        # Ø¥Ø­ØµØ§Ø¡ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
                        arabic_count = sum(
                            1 for q in self.questions if self.detect_language(q) == 'ar')
                        print(
                            f"ðŸ“Š Loaded {len(self.questions)} Q&A pairs from dataset ({arabic_count} Arabic)")
                        return
                    else:
                        print("âš ï¸ Dataset file exists but contains no valid data")
                else:
                    print("âš ï¸ Dataset file doesn't have enough columns")
                    
            except Exception as e:
                print(f"âš ï¸ Error loading dataset: {e}")
                # Ø­Ø§ÙˆÙ„ Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ù…Ù„Ù
                self.try_fix_dataset()
                return

        # ÙÙ‚Ø· Ø¥Ø°Ø§ Ø§Ù„Ù…Ù„Ù Ù…Ø´ Ù…ÙˆØ¬ÙˆØ¯ Ø£Ùˆ Ù…Ø´ Ù…Ù…ÙƒÙ† Ù†ØµÙ„Ø­Ù‡
        print("ðŸ“ Creating default dataset...")
        self.create_default_dataset()

    def try_fix_dataset(self):
        """Ù…Ø­Ø§ÙˆÙ„Ø© Ø¥ØµÙ„Ø§Ø­ Ù…Ù„Ù dataset Ø¨Ø¯Ù„ Ø­Ø°ÙÙ‡."""
        try:
            print("ðŸ› ï¸ Attempting to fix dataset file...")
            
            # Ø­Ø§ÙˆÙ„ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù Ø¨Ø·Ø±Ù‚ Ù…Ø®ØªÙ„ÙØ©
            try:
                df = pd.read_csv(self.dataset_file, encoding='utf-8', on_bad_lines='skip')
            except:
                try:
                    df = pd.read_csv(self.dataset_file, encoding='utf-8', sep=None, engine='python')
                except:
                    print("âŒ Could not read dataset file")
                    self.create_default_dataset()
                    return
            
            if len(df.columns) >= 2:
                # Ø®Ø° Ø£ÙˆÙ„ Ø¹Ù…ÙˆØ¯ÙŠÙ† ÙÙ‚Ø·
                df_fixed = df.iloc[:, :2].copy()
                df_fixed.columns = ['question', 'answer']
                
                # Ù†Ø¸Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                df_fixed = df_fixed.dropna()
                df_fixed['question'] = df_fixed['question'].astype(str)
                df_fixed['answer'] = df_fixed['answer'].astype(str)
                
                # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØµÙÙˆÙ Ø§Ù„ÙØ§Ø±ØºØ©
                df_fixed = df_fixed[(df_fixed['question'].str.strip() != '') & 
                                  (df_fixed['answer'].str.strip() != '')]
                
                if len(df_fixed) > 0:
                    # Ø§Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…ÙØµÙ„Ø­
                    df_fixed.to_csv(self.dataset_file, index=False, encoding='utf-8')
                    print(f"âœ… Fixed dataset file, keeping {len(df_fixed)} Q&A pairs")
                    
                    # Ø­Ù…Ù‘Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙØµÙ„Ø­Ø©
                    self.questions = df_fixed['question'].tolist()
                    self.answers = df_fixed['answer'].tolist()
                    return
            
            print("âŒ Could not fix dataset file, creating new one")
            self.create_default_dataset()
            
        except Exception as e:
            print(f"âŒ Error fixing dataset: {e}")
            self.create_default_dataset()

    def create_default_dataset(self):
        """Create default dataset ÙÙ‚Ø· Ø¥Ø°Ø§ Ù…ÙÙŠØ´ Ø¨ÙŠØ§Ù†Ø§Øª."""
        # Ø¥Ø°Ø§ ÙÙŠ Ø¨ÙŠØ§Ù†Ø§Øª Ù…ÙˆØ¬ÙˆØ¯Ø©ØŒ Ù…Ø§ ØªÙ†Ø´Ø¦Ø´ Ø¨ÙŠØ§Ù†Ø§Øª Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
        if len(self.questions) > 0 and len(self.answers) > 0:
            print("ðŸ“Š Using existing data, skipping default dataset creation")
            return
            
        data = {
            'question': [
                'Hello', 'Hi', 'How are you?', 'What is your name?', 'Goodbye', 'Thank you',
                'What can you do?', 'Help me', 'Good morning', 'Good evening',
                'Ù…Ø±Ø­Ø¨Ø§', 'Ø£Ù‡Ù„Ø§', 'ÙƒÙŠÙ Ø­Ø§Ù„ÙƒØŸ', 'Ù…Ø§ Ø§Ø³Ù…ÙƒØŸ', 'Ù…Ø¹ Ø§Ù„Ø³Ù„Ø§Ù…Ø©', 'Ø´ÙƒØ±Ø§',
                'Ù…Ø§Ø°Ø§ ØªØ³ØªØ·ÙŠØ¹ Ø£Ù† ØªÙØ¹Ù„ØŸ', 'Ø³Ø§Ø¹Ø¯Ù†ÙŠ', 'ØµØ¨Ø§Ø­ Ø§Ù„Ø®ÙŠØ±', 'Ù…Ø³Ø§Ø¡ Ø§Ù„Ø®ÙŠØ±'
            ],
            'answer': [
                'Hello! How can I help you today?',
                'Hi there! Nice to meet you!',
                'I am doing well, thank you! How are you?',
                'I am an AI chatbot here to help you.',
                'Goodbye! Have a great day!',
                'You are welcome! Happy to help!',
                'I can answer questions and have conversations in English and Arabic.',
                'I am here to help! What do you need?',
                'Good morning! Hope you have a wonderful day!',
                'Good evening! How can I assist you?',
                'Ù…Ø±Ø­Ø¨Ø§! ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø§Ù„ÙŠÙˆÙ…ØŸ',
                'Ø£Ù‡Ù„Ø§! Ø³Ø¹ÙŠØ¯ Ø¨Ù„Ù‚Ø§Ø¦Ùƒ!',
                'Ø£Ù†Ø§ Ø¨Ø®ÙŠØ±ØŒ Ø´ÙƒØ±Ø§ Ù„Ùƒ! ÙƒÙŠÙ Ø­Ø§Ù„ÙƒØŸ',
                'Ø£Ù†Ø§ Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ Ù‡Ù†Ø§ Ù„Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ.',
                'Ù…Ø¹ Ø§Ù„Ø³Ù„Ø§Ù…Ø©! Ø£ØªÙ…Ù†Ù‰ Ù„Ùƒ ÙŠÙˆÙ…Ø§ Ø±Ø§Ø¦Ø¹Ø§!',
                'Ø¹ÙÙˆÙ‹Ø§! Ø³Ø¹ÙŠØ¯ Ø¨Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ!',
                'ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© ÙˆØ¥Ø¬Ø±Ø§Ø¡ Ù…Ø­Ø§Ø¯Ø«Ø§Øª Ø¨Ø§Ù„Ù„ØºØªÙŠÙ† Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© ÙˆØ§Ù„Ø¹Ø±Ø¨ÙŠØ©.',
                'Ø£Ù†Ø§ Ù‡Ù†Ø§ Ù„Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ! Ù…Ø§ Ø§Ù„Ø°ÙŠ ØªØ­ØªØ§Ø¬Ù‡ØŸ',
                'ØµØ¨Ø§Ø­ Ø§Ù„Ø®ÙŠØ±! Ø£ØªÙ…Ù†Ù‰ Ù„Ùƒ ÙŠÙˆÙ…Ø§ Ø±Ø§Ø¦Ø¹Ø§!',
                'Ù…Ø³Ø§Ø¡ Ø§Ù„Ø®ÙŠØ±! ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒØŸ'
            ]
        }

        df = pd.DataFrame(data)
        df.to_csv(self.dataset_file, index=False, encoding='utf-8')
        self.questions = data['question']
        self.answers = data['answer']

        arabic_count = sum(
            1 for q in self.questions if self.detect_language(q) == 'ar')
        print(
            f"ðŸ“ Created default dataset with {len(self.questions)} Q&A pairs ({arabic_count} Arabic)")

    def train_model(self):
        """Train model using TF-IDF Ù…Ø¹ Ø¯Ø¹Ù… Ù…Ø­Ø³Ù† Ù„Ù„Ø¹Ø±Ø¨ÙŠØ©."""
        if not HAS_SKLEARN or not self.questions:
            print("âŒ sklearn not available or no questions")
            return False

        try:
            print("ðŸ”„ Training TF-IDF model with Arabic support...")

            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ØªØ¯Ø¹Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
            self.vectorizer = TfidfVectorizer(
                lowercase=False,  # Ù„Ø§ ØªØ­ÙˆÙ„ Ù„Ù„Ø£Ø­Ø±Ù Ø§Ù„ØµØºÙŠØ±Ø© Ø¹Ù„Ø´Ø§Ù† Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
                analyzer='char_wb',  # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø­Ø±Ù
                ngram_range=(2, 4),  # Ù†Ø·Ø§Ù‚ Ø£ÙˆØ³Ø¹ Ù„Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù†Ù…Ø§Ø·
                max_features=2000
            )

            self.embeddings = self.vectorizer.fit_transform(self.questions)
            self.is_trained = True

            print(f"âœ… TF-IDF model training completed")
            print(
                f"   - Vocabulary size: {len(self.vectorizer.get_feature_names_out())}")
            print(
                f"   - Arabic questions: {sum(1 for q in self.questions if self.detect_language(q) == 'ar')}")
            print(
                f"   - English questions: {sum(1 for q in self.questions if self.detect_language(q) == 'en')}")

            return True
        except Exception as e:
            print(f"âŒ Model training failed: {e}")
            return False

    def get_answer(self, question):
        """Get answer for a question using TF-IDF Ù…Ø¹ Ø¯Ø¹Ù… Ù…Ø­Ø³Ù† Ù„Ù„Ø¹Ø±Ø¨ÙŠØ©."""
        if not self.is_trained or not HAS_SKLEARN:
            if self.detect_language(question) == 'ar':
                return {
                    'answer': 'Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ØºÙŠØ± Ø¬Ø§Ù‡Ø² Ø¨Ø¹Ø¯.',
                    'confidence': 0.0,
                    'language': 'ar'
                }
            else:
                return {
                    'answer': 'Sorry, the model is not ready yet.',
                    'confidence': 0.0,
                    'language': 'en'
                }

        language = self.detect_language(question)
        print(f"ðŸ’¬ Processing: '{question}' (language: {language})")

        try:
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… TF-IDF Ø¨Ø¯Ù„ sentence-transformers
            question_vec = self.vectorizer.transform([question])
            similarities = cosine_similarity(question_vec, self.embeddings)[0]

            best_idx = np.argmax(similarities)
            best_confidence = float(similarities[best_idx])
            best_answer = self.answers[best_idx]

            print(
                f"ðŸŽ¯ Best match: {best_confidence:.3f} confidence -> '{best_answer}'")

            if best_confidence < self.confidence_threshold:
                if language == 'ar':
                    return {
                        'answer': 'Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ù… Ø£ÙÙ‡Ù… Ø³Ø¤Ø§Ù„Ùƒ.',
                        'confidence': best_confidence,
                        'language': 'ar'
                    }
                else:
                    return {
                        'answer': 'Sorry, I do not understand your question.',
                        'confidence': best_confidence,
                        'language': 'en'
                    }

            # Ø§Ù„ØªØ±Ø¬Ù…Ø© Ø¥Ø°Ø§ needed
            answer_language = self.detect_language(best_answer)
            final_answer = best_answer

            if not self.auto_translate and language != answer_language and self.translator:
                try:
                    translated = self.translator.translate(
                        best_answer,
                        dest='ar' if language == 'ar' else 'en'
                    )
                    if translated.text:
                        final_answer = translated.text
                        print(f"ðŸŒ Translated answer to {language}")
                except Exception as e:
                    print(f"âš ï¸ Translation failed: {e}")

            return {
                'answer': final_answer,
                'confidence': best_confidence,
                'language': language
            }

        except Exception as e:
            print(f"âŒ Error getting answer: {e}")
            if self.detect_language(question) == 'ar':
                return {
                    'answer': 'Ø­Ø¯Ø« Ø®Ø·Ø£ØŒ Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.',
                    'confidence': 0.0,
                    'language': 'ar'
                }
            else:
                return {
                    'answer': 'Sorry, an error occurred.',
                    'confidence': 0.0,
                    'language': 'en'
                }

    def generate_speech(self, text, language='en'):
        """Generate speech audio file."""
        if not HAS_GTTS:
            print("âŒ gTTS not available")
            return None

        try:
            # ØªØ­Ø¯ÙŠØ¯ Ù„ØºØ© gTTS Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù„ØºØ© Ø§Ù„Ù…ÙƒØªØ´ÙØ©
            tts_lang = 'ar' if self.detect_language(text) == 'ar' else language
            tts = gTTS(text=text, lang=tts_lang, slow=False)
            temp_file = tempfile.NamedTemporaryFile(
                delete=False, suffix='.mp3')
            tts.save(temp_file.name)
            print(f"ðŸ”Š Generated speech: {temp_file.name} (lang: {tts_lang})")
            return temp_file.name
        except Exception as e:
            print(f"âŒ Speech generation failed: {e}")
            return None

    def process_voice_input(self, audio_file_path):
        """Voice processing simulation - Ø§Ø³ØªØ®Ø¯Ø§Ù… Web Speech API ÙÙŠ Ø§Ù„Ù…ØªØµÙØ­."""
        print("ðŸŽ¤ Voice processing: Use browser Web Speech API")
        return "Voice recognition: Use browser speech API"


# Initialize chatbot
chatbot = ChatbotAPI()

# ÙƒÙ„ ÙˆØ§Ø¬Ù‡Ø§Øª API ØªØ¨Ù‚Ù‰ ÙƒÙ…Ø§ Ù‡ÙŠ Ø¨Ø¯ÙˆÙ† ØªØºÙŠÙŠØ±


@app.route('/api/health', methods=['GET'])
def health_check():
    return jsonify({
        'status': 'healthy',
        'model_ready': chatbot.is_trained,
        'questions_count': len(chatbot.questions),
        'arabic_questions': sum(1 for q in chatbot.questions if chatbot.detect_language(q) == 'ar'),
        'english_questions': sum(1 for q in chatbot.questions if chatbot.detect_language(q) == 'en'),
        'timestamp': datetime.datetime.now().isoformat()
    })


@app.route('/api/chat', methods=['POST'])
def chat_endpoint():
    try:
        data = request.get_json()
        if not data:
            return jsonify({'error': 'No JSON data provided'}), 400

        message = data.get('message', '').strip()
        if not message:
            return jsonify({'error': 'Message is required'}), 400

        result = chatbot.get_answer(message)
        response = {
            'answer': str(result['answer']),
            'confidence': float(result['confidence']),
            'language': str(result['language']),
            'timestamp': datetime.datetime.now().isoformat()
        }
        return jsonify(response)
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/speech', methods=['POST'])
def speech_endpoint():
    try:
        data = request.get_json()
        if not data:
            return jsonify({'error': 'No JSON data provided'}), 400

        text = data.get('text', '').strip()
        language = data.get('language', 'en')
        if not text:
            return jsonify({'error': 'Text is required'}), 400

        audio_file = chatbot.generate_speech(text, language)
        if audio_file:
            try:
                response = send_file(
                    audio_file, as_attachment=True, download_name='speech.mp3')

                @response.call_on_close
                def cleanup():
                    try:
                        if os.path.exists(audio_file):
                            os.unlink(audio_file)
                    except:
                        pass
                return response
            except Exception as e:
                if os.path.exists(audio_file):
                    os.unlink(audio_file)
                raise e
        else:
            return jsonify({'error': 'Speech generation failed'}), 500
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/voice-input', methods=['POST'])
def voice_input_endpoint():
    """Ø§Ø³ØªØ®Ø¯Ø§Ù… Web Speech API ÙÙŠ Ø§Ù„Ù…ØªØµÙØ­ Ù„Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØª."""
    return jsonify({
        'message': 'Use browser Web Speech API for voice recognition',
        'supported': True,
        'instruction': 'Use window.SpeechRecognition in browser'
    })


@app.route('/api/dataset', methods=['GET', 'POST'])
def dataset_endpoint():
    try:
        if request.method == 'GET':
            return jsonify({
                'questions': [str(q) for q in chatbot.questions],
                'answers': [str(a) for a in chatbot.answers],
                'count': len(chatbot.questions),
                'arabic_count': sum(1 for q in chatbot.questions if chatbot.detect_language(q) == 'ar'),
                'english_count': sum(1 for q in chatbot.questions if chatbot.detect_language(q) == 'en')
            })
        elif request.method == 'POST':
            data = request.get_json()
            if not data:
                return jsonify({'error': 'No JSON data provided'}), 400

            question = data.get('question', '').strip()
            answer = data.get('answer', '').strip()
            if not question or not answer:
                return jsonify({'error': 'Question and answer are required'}), 400

            chatbot.questions.append(question)
            chatbot.answers.append(answer)
            df = pd.DataFrame(
                {'question': chatbot.questions, 'answer': chatbot.answers})
            df.to_csv(chatbot.dataset_file, index=False, encoding='utf-8')
            chatbot.train_model()

            return jsonify({
                'success': True,
                'new_count': len(chatbot.questions),
                'language': chatbot.detect_language(question),
                'message': 'Question added successfully'
            })
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/status', methods=['GET'])
def status_endpoint():
    return jsonify({
        'model_trained': bool(chatbot.is_trained),
        'dataset_size': int(len(chatbot.questions)),
        'arabic_questions': sum(1 for q in chatbot.questions if chatbot.detect_language(q) == 'ar'),
        'english_questions': sum(1 for q in chatbot.questions if chatbot.detect_language(q) == 'en'),
        'features': {
            'text_to_speech': bool(HAS_GTTS),
            'speech_recognition': False,  # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…ØªØµÙØ­
            'translation': bool(HAS_TRANSLATOR and chatbot.translator is not None),
            'browser_voice_supported': True,
            'arabic_support': True
        }
    })


@app.route('/api/conversation', methods=['POST'])
def conversation_endpoint():
    try:
        data = request.get_json()
        if not data:
            return jsonify({'error': 'No JSON data provided'}), 400

        message = data.get('message', '').strip()
        get_speech = data.get('get_speech', True)
        if not message:
            return jsonify({'error': 'Message is required'}), 400

        result = chatbot.get_answer(message)
        response = {
            'answer': str(result['answer']),
            'confidence': float(result['confidence']),
            'language': str(result['language']),
            'timestamp': datetime.datetime.now().isoformat()
        }

        if get_speech and HAS_GTTS:
            audio_file = chatbot.generate_speech(
                result['answer'], result['language'])
            if audio_file:
                response['speech_url'] = f'/api/speech-file?text={result["answer"]}&lang={result["language"]}'

        return jsonify(response)
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/speech-file', methods=['GET'])
def speech_file_endpoint():
    try:
        text = request.args.get('text', '')
        lang = request.args.get('lang', 'en')
        if not text:
            return jsonify({'error': 'Text parameter is required'}), 400

        audio_file = chatbot.generate_speech(text, lang)
        if audio_file:
            response = send_file(
                audio_file, as_attachment=False, download_name='speech.mp3')

            @response.call_on_close
            def cleanup():
                try:
                    if os.path.exists(audio_file):
                        os.unlink(audio_file)
                except:
                    pass
            return response
        else:
            return jsonify({'error': 'Speech generation failed'}), 500
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/browser-voice-support', methods=['GET'])
def browser_voice_support():
    return jsonify({
        'browser_voice_supported': True,
        'text_to_speech': 'Use window.speechSynthesis',
        'speech_recognition': 'Use window.SpeechRecognition',
        'languages': ['en-US', 'ar-SA', 'en-GB'],
        'arabic_support': True
    })

# Error handlers


@app.errorhandler(404)
def not_found(error):
    return jsonify({'error': 'Endpoint not found'}), 404


@app.errorhandler(500)
def internal_error(error):
    return jsonify({'error': 'Internal server error'}), 500


@app.errorhandler(405)
def method_not_allowed(error):
    return jsonify({'error': 'Method not allowed'}), 405


if __name__ == '__main__':
    port = int(os.environ.get('PORT', 8000))
    print("ðŸš€ Starting Lightweight Flask API server...")
    print(f"ðŸŒ Server will run on port: {port}")
    print("ðŸ“¡ All API endpoints are available!")
    app.run(debug=False, host='0.0.0.0', port=port)
